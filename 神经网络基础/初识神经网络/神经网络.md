# 神经网络

### 一、简介

- ​	**特点和优点：**

  对于传统机器学习算法，比如：logical regression在处理特征值过多的问题时，会显得力不从心，这时就可以使用神经网络。再比如：对于普通特征难以满足预测的需要，需要通过复杂的组合得到一系列更为强大的新特征，从而完成回归或分类问题。

  在不产生梯度爆炸或梯度消失的前提下，构建深层次神经网络，在很多问题上会有更好的学习效果。他的模型也更灵活，通过一些变形和优化改造，可以衍生出我们熟知的一些深度学习算法，RNN、CNN等。



### 二、模型原理

- ​	**通过一个列子简单介绍一下：**

  以下是一个两层的神经网络，包含输入（Input Layer）、隐藏层（Hidden Layer）、输出（Output Layer）

  ![](https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-/blob/master/神经网络基础/assets/img1.png)

​	现在定义一个实际的问题：已知四个数据点(1,1)(-1,1)(-1,-1)(1,-1)，这四个点分别对应坐标卓的I~IV象限，如果这时候给我们一个新的坐标点（比如(2,2)），那么它应该属于哪个象限呢？（没错，当然是第I象限，但我们的任务是要让机器知道），此时Input Layer就是(x, y)的1\*2的矩阵, Output Layer就是1*4的矩阵（4个可预测值）。

​	**2.1 输入层**

​	上面的例子中，输入层是坐标值，例如（1,1），这是一个包含两个元素的数组，也可以看作是一个1\*2的矩阵。输入层的元素维度与输入量的特征息息相关，如果输入的是一张224\*224像素的灰度图像，那么输入层的维度就是(224, 224)。

------

​	**2.2 从输入层到隐藏层**

​	连接输入层和隐藏层的是W1 和 b1。由X计算得到H1十分简单，就是矩阵运算：

​																			`H1 = x * w1 + b1`	

​	假设隐藏层节点数是64，隐藏层为64维（也可以理解成64个神经元），矩阵H1的大小为（1\*64）的矩阵，则上述公式中，H1(1\*64) = x(1\*2) * w1(2\*64) + b1(64, )

------

​	**2.3 从隐藏层到隐藏层**			

​	连接输入层和隐藏层的是W2 和 b2， 则计算如下：

​																			`H2 = H1 * w2 + b2`		

​	具体公式分解：H2(1\*64) = H1(1\*64) * w2(64\*64) + b2(64, )	

------

​	**2.4 从隐藏层到输出层**											

​	连接输入层和隐藏层的是W3 和 b3， 则计算如下：

​																			`Y = H2 * w3 + b3`

​	具体公式分解：Y(1\*4) = H2(1\*64) * w2(64\*4) + b2(4, )	

------

​	**2.5 激活层**	

​		通过上述的几个线性的变换，可以求出最终的Y，但是几个线性的运算最终都是可以用一个线性的变换代替，也就是说上述的几个式子最终可以用一个线性的式子代替，不仅对于上述的神经网络是这样，就算网络深度加到100层，也依然是这样。这样的话神经网络就失去了意义。

​		所以激活函数的意义就在于此，将线性变换为非线性。

​		常见的激活函数介绍：

​						![](https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-/blob/master/神经网络基础/assets/Image 2.png)

​		**Sigmold激活函数**（一般用在二分类问题上，多分类问题一般用softmax）

​		优点：取值范围(0~1)、简单、容易理解

​		缺点：(1) 容易饱和和终止梯度传递(“死神经元”)；

​				   (2) sigmoid函数的输出没有0中心化；

​		公式：![](https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-/tree/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/assets/Image 3.png)

------

![](https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-/blob/master/神经网络基础/assets/Image 4.png)

​		**tanh激活函数**

​		优点：取值范围(-1~1)、易理解、0中心化；

​		缺点：容易饱和和终止梯度传递(“死神经元”)；

​		公式：![](https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-/blob/master/神经网络基础/assets/Image 5.png)

------

​									![](https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-/blob/master/神经网络基础/assets/Image 6.png)

​		**relu激活函数**（现在神经网络用的最多的，一般刚开始可以用）

​		优点：（1）相比于Sigmoid和Tanh，提升收敛速度；

​					（2）梯度求解公式简单，不会产生梯度消失和梯度爆炸；

​					（3）单侧抑制；

​					（4）相对宽阔的兴奋边界；

​		缺点：（1）没有边界，可以使用变种ReLU: min(max(0,x), 6)；

​					（2）比较脆弱，比较容易陷入出现”死神经元”的情况；

​		公式：![](https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-/blob/master/神经网络基础/assets/Image 7.png)

​		其他的激活函数，如Leaky ReLU， ELU等由于再实际场景中很少用到，就不做说明，想了解的可以自行百度。

------

​		**2.6 最终输出层：**

​		输出Y的值可能会是(3.5, 1, 0.2, 0.2)这样的矩阵，我们可以找到里边的最大值 “3.5”，从而找到对应的分类为I，但是这并不直观。我们想让最终的输出为概率，也就是说可以生成像(90%, 6%, 2%, 2%)这样的结果，这样做不仅可以找到最大概率的分类，而且可以知道各个分类计算的概率值。

​		这就是多分类问题需要使用的方法：**softmax**

​		公式：![](https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-/blob/master/神经网络基础/assets/Image 8.png)

​		简单来说分三步进行：（1）以e为底对所有元素求指数幂；（2）将所有指数幂求和；（3）分别将这些指数幂与该和做商。

​		这样求出的结果中，所有元素的和一定为1，而每个元素可以代表概率值。

​	

------

### 三、模型评估

​	指标：损失函数

​	通过Softmax层之后，我们得到了Y值对应的四个预测值的各自的概率，但是要注意，这是神经网络计算得到的概率值结果，而非真实的情况。

​	比如，Softmax输出的结果是(90%, 6%, 2%, 2%)，真实的结果是(100%, 0, 0, 0)。虽然输出的结果可以正确分类，但是与真实结果之间是有差距的，最好的网络效果式预测无限接近于100%，为此，我们需要将Softmax输出结果的好坏程度做一个“量化”。

​	对数损失就是应对的损失函数：

​	如：0.9 对应的对数损失就是 - log 0.9 =  0.046，可以发现，越接近100%， -logx就越接近0，也就是损失函数越接近于0，就说明结果越好，这种叫做：**交叉熵损失（Cross Entropy Error）**

​	训练网络的目的就是去不断的减少这个交叉熵损失

​	所以一个正常倍训练的网络的交叉熵损失应该像如下这样：

​	23 -> 13.3 -> 8.5 -> 4.6 -> 2.1 -> 0.7 -> 0.5 -> 0.3 ......一直到交叉熵损失函数的值不再更低为止

------

### 四、反向传播

​	神经网络的而反向传播就是不断更新参数，以使得损失函数值最小的过程，就拿梯度下降来说，假设现在某个人在山顶，他每次走的方向有六种可能， 分别是w1、w2、w3、b1、b2、b3，参数的具体值代表了具体走的距离，比如沿着w1方向走了1.2m，损失值变大了，那就走0.2m......一次类推，知道损失函数变小为止，其他五个参数的更新也是如此，当最终走到了山底，此时损失函数最小，网络达到最优。

------

### 五、代码案列

​	具体代码见链接：

[1]: https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-

​	iris数据集做分类，其中训练集和验证集的损失变化如下：

​	![](https://github.com/19960310/learn-neural-network-by-tensorflow-2.0-/blob/master/神经网络基础/assets/Image 9.png)
